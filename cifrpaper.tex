\documentclass[authoryear]{elsarticle}
\usepackage{latexsym}
%\usepackage{rotate}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\bibliographystyle{chicago}



\newcommand{\logit}{\mathrm{logit}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\p}{\mathrm{P}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\vecm}{\mathrm{vec}}
\newcommand{\kp}{\otimes}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\eps}{\epsilon}
\newcommand{\ep}{\varepsilon}
\newcommand{\obdots}{\ddots}    % change this later
\newcommand{\Ex}{{\cal E}}
\newcommand{\rat}{{\frac{c_{ij}}{c_{i,j-1}}}}
\newcommand{\rmu}{m}
\newcommand{\rsig}{\nu}
\newcommand{\fd}{\mu}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\cor}{\mathrm{cor}}
\newcommand{\bx}[1]{\ensuremath{\overline{#1}|}}
\newcommand{\an}[1]{\ensuremath{a_{\bx{#1}}}}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\renewcommand{\i}{\item}
\newcommand{\sr}{\ensuremath{\mathrm{SRISK}}}
\newcommand{\cs}{\ensuremath{\mathrm{CS}}}
\newcommand{\cri}{\ensuremath{\mathrm{Crisis}}}
\newcommand{\var}{\ensuremath{\mathrm{VaR}}}
\newcommand{\covar}{\ensuremath{\mathrm{CoVaR}}}
\newcommand{\med}{\ensuremath{\mathrm{m}}}
\newcommand{\de}{\mathrm{d}}
\renewcommand{\v}{\ensuremath{\mathrm{v}_q}}
\newcommand{\m}{\ensuremath{\mathrm{m}}}
\newcommand{\tvar}{\ensuremath{\mathrm{TVaR}}}



\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\fref}[1]{Figure \ref{#1}}
\newcommand{\sref}[1]{\S\ref{#1}}
\newcommand{\tref}[1]{Table \ref{#1}}
\newcommand{\aref}[1]{Appendix \ref{#1}}




\newcommand{\cq}{\ , \qquad}
\renewcommand{\P}{\mathrm{P}}
\newcommand{\Q}{\mathrm{Q}}

\newcommand{\Vx}{{\cal V}}
\newcommand{\be}[1]{\begin{equation}\label{#1}}
\newcommand{\ee}{\end{equation}}




\begin{document}

% Title of paper
\title{Systemic risk and contagion effects in Australian financial institutions and sectors}
% List of authors, with corresponding author marked by asterisk
\author{Piet de Jong,  Geoff Loudon and Weihao Choo \\[4pt]
% Author addresses
\textit{Department of Applied Finance and Actuarial Studies\\ Macquarie University, Sydney, NSW 2109.}
\\[2pt]
%E-mail address for correspondence
{piet.dejong@mq.edu.au}}

% Running headers of paper:
\markboth%
% First field is the short list of authors
{De Jong}
% Second field is the short title of the paper
{Systemic risk}

\maketitle

\section{Literature review}

The starting point for the proposed research is the recent literature and the CIFR targeted areas and APRA aims and functions.
This recent literature includes the following
\cite{adrian2011covar},
\cite{acharya2012capital},
\cite{acharya2012measuring}
and \cite{brownlees2010volatility}.   The proposed research aims to extend and apply these techniques particularly in relation to the entities regulated by APRA.   Thus our  broad aim is to develop, implement and bring to bear recent developments in stress testing  on the aims of APRA and the CIFR targeted research areas detailed above.   

\section{Improved  measures of contagion and systematic risk}
\renewcommand{\c}{\ensuremath{\mathrm{CoVaR_q}}}
\renewcommand{\v}{\ensuremath{\mathrm{VaR}_q}}

$\covar_q$ as proposed in \cite{adrian2011covar} is a basis for proposed measures contagion, exposure and systemic risk.   It  suffers from a number of drawbacks:
\bi
\i Couched in terms of $\var_q$ containing the scale of the original measurements.   It is worthwhile to have measures and techniques robust to scale.
\i  Conditioning  on $\var_{0.5}$ is undesirable and relatively intractable.  In our proposal we reference stress with respect  to the unconditional $\var_q$.   This permits a more transparent analysis and estimation. 
\i  Our proposed approach  separates out scale effects and interdependence effects and aims to  relates these separately to external variables including shocks and drivers of systemic risk.   Thus $\var_q$ movements due to scale are disentangled from movements due to codependence with separate driver responses.
\ei

\section{Significance of the project and  policy implications}

Understanding the impact of external shocks and their propagation through   the financial system is vital for managing and remediating systemic risk. Effective regulation is dependent upon the development of a robust and reliable set of appropriate risk measures.  We propose new measures of systemic risk that relate marginal and joint distributions separately to external drivers. This allows for more cogent and coherent stress testing as it includes the estimation of contagion effects, exposure effects and systemic risk across related entities and different financial sectors. Improved stress testing, estimation of risk effects and transmission of shocks through the financial system will make for more cogent prudential policy, prudential margin setting and better identify sources of risk to the financial system.

\newcommand{\q}{\mathrm{Q}}
\section{Percentile sensitivity and contagion}\label{perc}

\subsection{Theorem}  Suppose $x$ is a random vector with marginal distributions\footnote{To economise on notation, write $F_j(x)\equiv F_j(x_j)\equiv F(x_j)$ and similar for other vector functions.} 
$$
F(x)\equiv \{F(x_1),\ldots ,F(x_p)\} \equiv (u_1,\ldots,u_p)\equiv u\ .
$$ 
Further suppose $0\le q\le 1$ is given and  $\q(x)$ is the vector of $q$--quantiles
$$
F\{\q(x)\}=q1=\q(u)\ ,
$$
where $1$ is a vector of $p$ ones. 
Define the stress vector with respect to $x_j$ as\footnote{In \cite{adrian2011covar}  $\Delta CoVar_q\equiv q_{y|x=q_x}-q_y$.  Variable $y$  is generally the ``financial system" and hence considered is the change in the \v\ of the system when institution $x$ stressed, with stress  interpreted as $x=q_x$.  On page 10 of their paper they incorrectly state ``... $CoVaR$ conditions on the event that [an] institution is at its VaR level, which occurs with probability $q$."} 
\be{stress}
\frac{\de\q(x)}{\de x_j} \equiv \q(x|u_j>q) - \q(x)\ ,
\ee
where $\q(x|u_j>q)$ is the vector of $q$--quantiles of $x$ given $u_j>q$.    Then if $\q(x)$  is linear in $q$ 
 then
\be{implicit}
\frac{\de\q(x_i)}{\de\q(x_j)} \equiv \frac{\de\q(x_i)/\de x_j}{\de\q(x_j)/\de x_j} =  
\frac{f_j}{f_i} 
 \frac{q_{ij}}{q(1-q)}\cq q_{ij}=C_{ij}(q+q_{ij},q) - q^2\ ,
\ee
where $f_i$ and $f_j$ are the densities of $x_i$ and $x_j$ evaluated at $\q(x_i)$ and $\q(x_j)$ and  $C_{ij}$ is the copula of $(u_i,u_j)$.   
Further if 
$$
s_{ij}\equiv \frac{q_{ij}}{q(1-q)}\ ,
$$ then  $-1\le s_{ij}\le 1$ with $s_{ij}=\pm 1$ if $x_i$ and $x_j$ are comonotonic and counter monotonic, respectively.  If $x_i$ and $x_j$ are independent then  $s_{ij}=0$. If $u_i$ and $u_j$ are exchangeable  then  $s_{ji}=s_{ij}$.

\subsection{Proof}

By definition
$$
\frac{\de\q(u_i)}{\de u_j} \equiv \q(u_i|u_j>q) -q \equiv q_{ij}\ ,
$$
where  $q_{ij}$ is such that
\be{Qdef}
q =  \frac{\P(u_i\le  q+q_{ij},u_j>q)}{1-q}  = \frac{ q+q_{ij}-C_{ij}(q+q_{ij},q)}{1-q}\ ,
\ee
Rearranging yields the second equation in \eref{implicit}.
Now
 \be{first}
 \frac{\de\q(x_i)}{\de x_j}\equiv \q_{q+q_{ij}}(x_i) - \q(x_i) \approx \q'(x_i)q_{ij}= \frac{q_{ij}}{f_i} \ ,
 \ee
where $'$ denotes differentiation with respect to $q$ and the subscript on $\q$ indicates the revised $q$ for the quantile calculation.  The approximation follows from a first order Taylor expansion and is exact if the quantile is linear in $q$.   Similarly
 \be{second}
 \frac{\de\q(x_j)}{\de x_j}\equiv  \q_{q+q(1-q)}(x_j) - \q(x_j)\approx \frac{q(1-q)}{f_j}\ .
 \ee
 Dividing \eref{first} by \eref{second} yields the first equation in  \eref{implicit} and completes the proof.

\subsection{Discussion}  The critical  result is that sensitivities factor into contributions from the ratios of the marginal densities and quantities calculated from the pairwise copulas.  The via  the implicit  equation for $q_{ij}$ in \eref{implicit}, solved using a  root finding algorithm.
  The quantities $q_{ij}$ are implicitly defined from the pairwise copulas. In summary the ``sensitivity" matrix 
$$
S \equiv \frac{\de Q(x)}{\de Q'(x)}\ ,
$$
where $'$ denotes transposition has ones on the diagonal quantities between $\pm 1$ off the diagonal.   The matrix $S$ is called the sensitivity matrix and displays the sensitivity of the $q$--quantile of  each component of $x$ to  stress in the same or other components.  

Since $f_i$ and $f_j$ are  the densities evaluated at $\q(x_i)$ and $\q(x_j)$  it follows that the ratio $f_i/f_j$ can be replaced by the ratio of the hazards.   This is  useful in cases were it is practical to model the hazard rather than the density.


\subsection{Implementation}

\fref{fig1} displays the empirical copulas calculated from four weekly closing stock prices labelled anz, cba, mcq and  wbc for $n=761$ weeks from 2000 April 12 through to 2014 October 29.  The empirical copulas are calculated by converting each observation to an empirical percentile and plotting the same against each of the other series percentiles.  

\begin{figure}
  \begin{center}
    \includegraphics{fig1.pdf}
    \caption{Pairwise copulas of bank stocks  cba, anz, mqg, wbc, and the overall bank index.  Red lines plot sensitivity $s_{ij}$ as a function of $q$.}\label{fig1}
   \end{center}
\end{figure}


To estimate  $q_{ij}$  at a particular $q$, the second equation in \eref{implicit} is iterated\footnote{The second equation in \eref{implicit} is a contraction mapping and hence has a fixed point.} starting from $q_{ij}=0$ where copulas are estimated as 
$$
\hat C_{ij}(u_i,u_j) \equiv \hat\E\{(p_{ik}\le u_i)(p_{jk}\le u_j)\}\ .
$$
Here $\hat\E$ computes the empirical mean over the cases $k=1,\ldots,n$ and $p_{ik}$ and $p_{jk}$ are the empirical percentiles  of case $k$ of $x_i$ and $x_j$, respectively.

\subsection{Generalisations}
\renewcommand{\r}{\mathrm{R}}
Similar results apply when $\Q(x)$ is replaced by  other risk measures such as $\r(x)\equiv\E\{xr(u)\}$ where $r$ is a given function which acts componentwise and $xr(u)$ denotes componentwise multiplication.  For example if $r(u)=mu^{m-1}$ then  $\r(x)=\E\{\max(x^1,\ldots,x^m)\}$ where  $x^1,\ldots,x^m$ are $m$ independent copies of $x$ and the risk measure is the expected worst outcome in $m$ independent trials.

With $\r(x)$, the analogue of \eref{stress} is 
\be{stress2}
\frac{\de\r(x)}{\de x_j} \equiv \r(x|u_j>r_j) - \r(x_j)\cq r_j\equiv\p\{x_j\le\r(x_j)\} \ .
\ee
This differs from \eref{stress}  in that a different risk measure is used and  $r_j$ replaces $q$.   If  the components of $\r(x)$ are linear in the $r_i$ then
\be{implicit2}
\frac{\de\r(x_i)}{\de\r(x_j)} =  
\frac{f_j}{f_i} 
 \frac{q_{ij}}{q(1-q)}\cq q_{ij}=C_{ij}(r_i+q_{ij},r_j) - r_ir_j\ ,
\ee
where $f_i$ and $f_j$ are the $x_i$ and $x_j$ densities at the $r_i$ and $r_i$ quantiles, respectively. 

\section{Systemic risk}

Suppose a firm at time $t$ has debt $D_t$ and equity $W_t$.   Then at $t$, $D_t+W_t$ is the total claim on the firm. 
Regulated firms are generally required to hold a proportion $k$ of claims $k(D_t+W_t)$ as a prudential margin.    If  $k(D_t+W_t)>W_t$ then, prudentially speaking, the firm has capital shortfall   $k(D_t+W_t)-W_t$.  Negative capital shortfall indicates positive working capital.

Assume $D_t=D_{t+1}=\cdots=D_{t+h}$ where $h$ is integer,  and 
$$
W_{t+h} = (1+i_{t+1}+\cdots+i_{t+h})W_t=(1+r_{t})W_t\ ,
$$
serving to define the firm return $r_t$.  Note $r_t$ is the  $h$ period arithmetic return up to  time $t+h$.  
Then the capital shortfall at time $t+h$ is 
$$
S_t\equiv  k(D_{t+h}+W_{t+h})-W_{t+h} = k D_{t} - (1-k)(1+r_{t})W_t
$$
\be{cs}= \left\{k\ell_t-(1-k)r_{t} - 1\right\}W_t\cq \ell_t\equiv\frac{D_t+W_t}{W_t}\ ,
\ee
where $\ell_t$ is the leverage ratio.  Note  $S_t$ is a linear in $r_t$, the firm rate of return.


A systemic event or ``crisis"   occurs in the interval $(t+1,t+h)$ if the market  return $r_{mt}\le c$ over ``the" period.   The crisis distribution of $S_t$ is the distribution of capital shortfall given $r_{mt}\le c$.  The systemic risk of  a firm is the expectation of the capital shortfall given  a crisis:
\be{sr}
\rho_t\equiv \E(S_t|r_{mt}\le c) \equiv \E^c(S_t) =  \{k\ell_t-(1-k)\E^c(r_{t})-1\}W_t\ .
\ee
The condition $D_{t+h}=D_t$ or less strongly $\E_t^c(D_{t+h})=D_t$ indicates the value of debt  does not change if there is a crisis.   

Evaluating the systemic risk in a firm requires the evaluation or estimation of the ``crisis" expectation $\E^c(r_{t})$ also called the long run marginal expected shortfall (LRMES).  Systemic risk is a point estimate of capital shortfall in a crisis.   Since $S_t$ is linear in $r_t$ the crisis distribution of $S_t$ is known if the conditional distribution of $r_t|r_{mt}<c$ is known.      
  

Given a number of firms,  $\rho_t$ is vector with components $\rho_{it}$ corresponding to  firm $i$.   Aggregate systemic risk is then $1'\rho_t$.  The proportion of systemic risk in firm $j$ is  $\rho^+_{it}/(1'\rho_t)$ where $\rho^+_{it}$ is the positive part of $\rho_{it}$.

An econometric models  links  returns $r_{it}$  to the  market return $r_{mt}$ enabling  the calculation of the crisis expectations 
$$
\E^c(r_{it}) \equiv \E(r_{it}|u_{mt}\le c^*)\cq c^*=\E\{(r_{mt}\le c)\}\ ,
$$
where $u_{mt}$ is the percentile of $r_{mt}$.
Given a joint distribution for $(r_{it},r_{mt})$ the distribution is simulated and
\be{sim}
\E^c(r_{it}) \approx \frac{\hat \E\{(r_{mt}\leq c)r_{it}\}}{\hat \E\{(r_{mt}\leq c)\}}= \frac{\hat\E\{(u_{mt}\leq c^*)F^-_{it}(u_{it})\}}{c^*}\ ,
\ee
where the $r$ values in the middle expressions denote simulated values and $\hat\E$, averages over the simulated values.   The $u_{it}$ and $u_{mt}$ are the percentiles of $r_{it}$ and $r_{mt}$, respectively.

The right hand expression in \eref{sim}  suggests using a copula to model the joint distributions $(r_{it},r_{mt})$.  Simulate $(u_{it},u_{mt})$, from the copula, discarding pairs with $u_{mt}>c^*$.   For each accepted $(u_{it},u_{mt})$  determine the actual return using the respective marginal distributions.   Issues to be addressed are:
\bi
\i Are the marginal distributions time invariant?  Could have situation where the marginal distribution evolves over time and/or is state dependent
\i Is the copula time invariant?   Could have situation where the copula is state dependent.    
\ei


The first step here is to examine the marginal distributions of the returns and the empirical copula of returns.

\section{Empirical studies}
\cite{brownlees2015} use  data on 95 large US financial firms for the period 2000-2012:   daily equity returns and market capitalization from CRSP; quarterly book values for equity and debt from Compustat.   A   Dynamic Conditional Correlation (DCC) model is estimated for  firm and market returns.  Write
\newcommand{\veps}{\varepsilon}
$$
\cov (r_t|t) = V_tR_tV_t'\cq R_t\equiv \cov(\veps_t|t)\cq \veps_t=V_t^{-1}r_t\ ,
$$
where $V_t$ is the diagonal matrix of volatilities at time $t$ and $R_t$ is the correlation matrix at time $t$.   Volatilities in $V_t$  are estimated using univariate threshold GARCH models.  The estimate of $R_t$ is the  correlation matrix corresponding to fitted covariance matrix $C_t$,  fitted using the model
$$
(C_t - C) = \alpha (C_{t-1}-C)+\beta (\veps_t\veps'_t-C) \cq C\equiv\cov(\veps_t)\ .
$$

\cite{brownlees2015} find the above systemic risk measure  a)  successfully identifies systemically risky firms during the GFC;
b)  predicts capital injections by Fed Reserve; and c) aggregate systemic risk provides early warning of declines in industrial production and higher unemployment.

{\bf Comment.}  With the DCC model both volatilities and correlations have a time dependent structure.  These are modelled separately.
An alternative approach is where 

\section{ARCH specification}

We have $u_{it}$, $u_{mt}$ and model the later as
$u_{t} = \Phi(z_t)$ where $z_t$ is standard normal.   A GARCH(1,1) model stated in terms of noise $\eps_t\sim(0,1)$ and time varying volatility $\sigma_t$:
$$
y_t=  \sigma_t\eps_t\cq \sigma^2_{t+1} = \alpha_0+\alpha_1y^2_{t}+ \beta_0 \sigma_t^2\cq \eps_t\sim N(0,1)\ .
$$
\begin{equation}\label{garch}
z_t=  \sigma_t\eps_t\cq (\sigma^2_{t+1}-1) = \alpha (\sigma^2_{t}-1) + \beta(z_t^2-1)\cq \eps_t\sim N(0,1)\ .
\end{equation}
$$
r_{mt}=(F_m^-\circ\Phi)(\sigma_t\eps_t)
$$
Since $\E(z_t^2)=1$ and $\alpha<1$, then in the long run $\sigma^2_t\rightarrow 1$.   If $z_t$ is in either tail and hence $z_t^2$ is far from  1 then $\sigma^2_{t+1}$ is much bigger than 1 and $z_{t+1}$ will be in the tail.   Then
\be{forward}
r_{it}=F_i^-(u_{it})\cq u_{it}\sim C\{u_{it}|\Phi(z_t)\}
\ee
Thus volatility clustering occurs since $z_t$ in one or tail, drives, via the copula, both $r_{it}$ and $r_{jt}$ into the tail.      

Steps in fitting:
\begin{enumerate}
\i  An ARCH model is fit to the market return $z_t=(\Phi^-\circ F_m)(r_{mt})$ where $F_m$ is the empirical distribution of the market rates
\i  The $z_t$ is simulated forward from time $t=n$ using the ARCH model. 
\i  Retain only those forward simulations such that the cumulative drop $\prod_{t=n+1}^N\{1+(F_m^-\circ \Phi)(z_t)\}$ is less than the crisis drop.
\i  For each forward simulated value, chose $r_{it}$ using the conditional copula and simulation smoothing
\i Calculate the capital shortfall for firm $i$.
\end{enumerate}

   
The rationale is that  $(y_{t}-\mu)^2$ is the latest available estimate of the variance at time $t$ and this latest   estimate is combined with the  ``permanent" component $\sigma^2_{t}$.   Constraints  are required on $a_0$, $a_1$ and $b_1$ to ensure $\sigma_t$ stays positive and finite.  If  $a_1=b_1=0$ then $y_t$ is noise with mean $\mu$. 

In our case $y_t=z_t$ is standard normal so we can drop $\mu$ to get
$$
z_t=  \sigma_t\eps_t\cq \sigma^2_{t+1} = a_0+a_1 \sigma^2_{t} + b_1z_t^2\ .
$$ 


Fitting GARCH models with R.  The  command $garch(y,c(1,1))$ fits  model $garch$ to $y$.  The $garch$ command requires $library(tseries)$. 

The quantity $a_0/(1-a_1-b_1)$ is the long run average $\sigma^2_t$ since it is the constant solution to the right hand side equation \eref{garch}, if  $(y_t-\mu)^2$ is replaced by its expectation.    We want the long run average to be 1 implying
$
a_0=1-a_1-b_1
$ and hence
$$
z_t=  \sigma_t\eps_t\cq \sigma^2_{t+1} = a_0+a_1 \sigma^2_{t} + (1-a_0-a_1)z_t^2\ .
$$
$$
z_t=  \sigma_t\eps_t\cq (\sigma^2_{t+1}-1) = a_1 (\sigma^2_{t}-1) + (1-a_0-a_1)(z_t^2-1)\ .
$$



 \fref{fig7} displays output from fitting a GARCH model to the daily percentage returns of the FTSE index plotted in \fref{fig6}.   In this fit  $\mu$ is set to the sample mean return, $\hat a_1=0.045$ indicates moderate autocorrelation in the standard deviation while $\hat b_1= 0.942$ indicates high reaction to the latest volatility $(y_t-\mu)^2$.  The long run average standard deviation is $\sqrt{0.009/(1-0.045-0.942)}=0.825$, marginally above the empirical standard deviation of 0.796.   The  standard errors of the coefficients are 0.003, 0.007 and 0.010, respectively for $a_0$, $a_1$ and $b_1$.  These are available using the $vcov(r)$ where $r$ contains the results of the $garch$ command.  Hence all three coefficients are significant.

GARCH stands for generalized ARCH where the latter stands for autoregressive conditional heteroskedasticity.    GARCH(1,1) indicates one lag in both the $\sigma_t^2$ and $(y_t-\mu)^2$ terms.   The GARCH($p,q$) model extends this to $p$ and $q$ lags.   The  ARCH($q$) model contains only lags in $(y_{t}-\mu_t)^2$  and hence is GARCH($0,q$).   
    
Estimating GARCH models is similar to ARMA models.   For any set of $a$ and $b$ parameters as well as starting conditions,  $y_0$ and $\sigma_0$,   the GARCH equation for $\sigma_t^2$  is used to recursively calculate $(y_t-\hat y_{t})/\sigma_t$.  These standardised errors, as well as related information,  are combined to evaluate the normal based likelihood.   The likelihood is maximised  with respect to the unknown parameters using a numerical search. 

Forecasting future volatility with  a GARCH model consists of iterating the $\sigma_t^2$ equation \eref{garch} forward from the latest available time point $t=n$.    In this iteration, future $(y_t-\mu_t)^2$ are replaced by their latest conditional expectation. 



\section{Partially Gaussian copulas}

Suppose $(u,v)=F_*(x,y)$  where $x$ and $y$ are scalar random variables.    Suppose $0<u_1<\cdots<u_n<1$  are   ordered observed values of $u$ and $q_j=\Phi^-(u_j)$, $t=1,\ldots,n$.  The correspondingly ordered values of $v$ and $z=\Phi^-(v)$ are denoted $v_j$ and $z_j$.  Note $\Ex(z_j)=\Ex(q_j)=0$ and $\Ex(z^2_j)=\Ex(q^2_j)=1$ where $\Ex$  is the empirical mean. 

Smoothing methods are here proposed to smooth and simulate $y$ values  using a copula on $(u,v)$ constructed via    a cubic spline model linking $u$ to $z$.   Of particular concern are simulated $z$ when $u$ is  near $0$ or $1$ corresponding to  $x$ extremes.    Given a simulated $u$ and in turn $z$, $(x,y)=F^-_*\{u,\Phi(z)\}$, a draw from the joint.  Constraining $u$ draws to say the upper tail, yields corresponding upper tail values for $y$.   The challenge is to produce cogent $z$'s or $y$'s that properly reflect dependence in different parts of the distribution unencumbered by inappropriate constraints, and in particular tail constraints, inherent in say the Gaussian copula setup.    Practical methods admit fitting on the basis of observed data, and testing procedures for  departures or otherwise from Gaussianity.   The method outlined below achieves this.

\begin{comment}
   The simulation singles out independent and dependent variables.   Which should be which?  If $x$ and $y$ are actual time series the simulation $(x_j,\hat y_j)$ is ordered back into the original time series order.
\end{comment}

The partially Gaussian  (PG) copula for $(u,v)$ is stated in terms of  $q\equiv\Phi^-(u)$ and $z\equiv\Phi^-(v)$ on the grid   $u_j=j/(n+1)$, $j=1,\ldots,n$ with 
\be{cusp}
z_j = \alpha_j+\beta q_j+ sh_j\eps_j\cq \alpha_{j+1}=\alpha_j +\delta_j+c\eta_j\cq \delta_{t+1}=\delta_j+\eta_j\ .
\ee
Here $\eps_j$ and $\eta_j$ are serially and contemporaneously uncorrelated zero mean disturbances with common variance $\sigma^2$ and $c=3-\sqrt 3$. The  $q_j$ term is the   Gaussian component and  $\alpha_j$ a nonparametric deviation.   

The parameter $s$ controls overall smoothness while  $h_j\ge 0$,  controlling the variability at different $t$ determines local smoothness.
In particular if $s=0$ then the signal $\alpha_j+\beta q_j$ is forced to reproduce every  $z_j$ while $h_j=0$ implies reproduction at the particular $t$.     If $h_j$ is large then little notice is taken of $z_j$ and the fitted signal at $t$ is relatively smooth.  Thus $s$ controls overall smoothness of the signal and  $h_j$ the smoothness at a particular $t$.  Comparability between models and fits is facilitated by normalising $h_j>0$ such that the average  is 1.  


The Gaussian copula is where $\alpha_j$ and $h_j$ are  identically 0 and  1, respectively, implying  $(z_j,q_j)$ with $t$ uniform on $1,\ldots,n$  is bivariate normal  with correlation $\beta=\pm\sqrt{1-(s\sigma)^2}\equiv\rho$.    

The starting condition for the recursion in \eref{cusp} are the second two equations with $t=0$ and where $\alpha_0$ and $\delta_0$ are  unknown parameters, similar to $\beta$. If $s\rightarrow\infty$ then $\sigma\rightarrow 0$,  $(\alpha_j,\delta_j)\rightarrow 0$ for $t=0,\ldots n$, and \eref{cusp} describes a Gaussian copula.  

The function $\alpha_j$ is thought of as an intercept function and provides for a nonparametric extension to the Gaussian copula.  If $\sigma\rightarrow 0$, equivalent to $s\rightarrow\infty$,  the first equation in \eref{cusp} is
$
z_j = \alpha_0+\delta_0 t + \beta q_j + s\eps_j
$
and  $\beta=\rho$ if $\alpha_0=\delta_0=0$.    Model \eref{cusp} with  $\beta=0$ is the  nonparametric model since $z_j$ is varies smoothly with $t$, unrelated to $q_j$.  If $\beta=0$ and $\sigma\rightarrow 0$ there is a probit relationship between $z_j$ and $u_j$.  Finally $s=0$ implies $\alpha_j+\beta q_j=z_j$ for any $\beta$.   The full range of possibilities is set out in \tref{pg} discussed in more detail below.  

The $\alpha_j=0$ and $\alpha_j=\alpha_0+\delta_0 t$ specifications are generalised by supposing $\alpha_j$ is twice differentiable and minimising
\be{spline}
 \sum_{t=1}^n\left(\frac{z_j-\alpha_j-\beta q_j}{h_j}\right)^2 + s^2\int_{-\infty}^{\infty} \left(\alpha_j''\right)^2\de t\ .
\ee
with respect to $\alpha_j$ and $\beta$.
 In \eref{spline} the $h_j$  are fitting weights: a large  $h_j$ implies  error at $t$ is largely ignored while a small $h_j$ implies  error at $t$ is  minimized at all cost.  The function $a_j$ and scalar $b$ minimising \eref{spline} deliver a function $a_j+bq_j$ where $a_j$ is piecewise twice differentiable and $a_j+bq_j$ passes closest to, in a mean square error sense, to $z_j$  with a penalty $s^2$ for  roughness measured in terms of the second derivative.   At the minimum, the first term in \eref{spline} is $(s\sigma)^2$.   \cite{Brown&DeJong:2001} demonstrate the equivalence of \eref{cusp} and \eref{spline}.

The  function $\alpha_j$ minimising \eref{spline}  is, at integer $t$,  $a_j=\E(\alpha_j|z_1,\ldots,z_n)$  computed with \eref{cusp} where $\eps_j$ and $\eta_j$ are normal, 0 mean, variance $\sigma^2$ random variables and $q_j$ fixed.  Between integer $t$,  the  minimum is quadratic in $t$.  A useful feature is that $\beta$ is simultaneously estimated alongside $a_j$ as the generalised least squares estimate under \eref{cusp}.  All calculations under \eref{cusp} provide error covariance matrices as detailed and applied below.  

\section{Special cases of the PG model}  

Special cases of  \eref{cusp}  are displayed in \tref{pg}.   In \eref{cusp}, if $s\rightarrow\infty$ then $\sigma\rightarrow 0$   since the variance of $z_j$ is finite.    Thus $s\rightarrow\infty$  implies the noise to signal ratio becomes large where noise refers to   $s\eps_j$ and signal to $\eta_j$, the error term driving  the smooth component $\alpha_j$.   As the noise to signal ratio $s$ increases, less of the variation in $z_j$ is explained by $\alpha_j$.  In the limit $\sigma$ is zero implying $\alpha_j=\alpha_0+\delta_0 t$, as straight line.   The further restriction $\alpha_0=\delta_0=0$ forces $\alpha_j$ identically zero.



The first row in \tref{pg} is the  Gaussian copula.  In this setup $\alpha_j$ is identically zero: there is no  signal component.  The additional restriction $\beta=0$ eliminates the Gaussian component $q_j$.  Co (or counter) monotonicity results in the Gaussian framework if $\rho=\pm 1$ as displayed in the third row.
If $s= 0$ then there is no measurement noise and the combined signal $\alpha_j+\beta q_j$ must pass through the points $z_j$ resulting in a perfect fit.   In terms of \eref{spline}, $s=0$ implies there is no roughness penalty and hence the minimisation is solely focussed on minimising the first, squared error, term, achieved  by setting $\alpha_j+\beta q_j=z_j$. 
 
 \begin{table}[htdp]
\caption{Special cases of the PG copula; in all cases $\rho\equiv\pm\sqrt{1-(s\sigma)^2}$}\label{pg}
\begin{center}
\begin{tabular}{l|c|ccccc|l}
\hline
case& $\alpha_j$ & $\beta$ &$s$ & $\sigma$ & $\alpha_0$&$\delta_0$  & other\\
\hline
Gaussian & 0 &  & $\infty$ & $0$&  0&0 &$\rho=\beta$\\
independence &  0 & 0 & $\infty$&0 & 0 &0 &$\rho=0$\\
$\pm$comonotonic  & 0 &$\pm 1$ &0 & 0 &0 & 0 &$z_j=\pm q_j$  \\
empirical  & $z_j$ &  & $0$  &  &   &  &$\rho=1$ \\ 
linear probit &$\alpha_0(1-2u_j)$&  0 &  $\infty$ &0& & $\frac{-2\alpha_0}{n+1}$ &indep. if $\alpha_0=0$\\
nonparametric & $\E(z_j|u_j)$  & 0 & & & &  & non Gaussian \\
PG & $\E(z_j-\beta q_j|u_j)$ & & & & & &  \\
\hline
\end{tabular}
\end{center}
\end{table}%

The linear probit row in \tref{pg}  eliminates the Gaussian component $q_j$ by setting $\beta=0$, and giving maximum penalty for rougness.   The resulting prediction  is a straight line $\hat z_j=\alpha_0+\delta_0 t$.  However the mean of the $z_j$ and hence $\hat z_j$ is zero implying $0=\alpha_0 + \delta_0(n+1)/2$.   Solving for $\delta_0$ gives the $\delta_0$ constraint displayed in \tref{pg} and the resulting straight line signal $\alpha_j$.

The bottom row of \tref{pg} displays the general partial Gaussian specification absent  any constraints on the parameters.  Eliminating the Gaussian component from the signal by setting $\beta=0$ leads to the nonparametric, non Gaussian specification in the second last row of \tref{pg}.

\section{Assessing goodness of fit}

A plot of $a_j+b q_j$ versus $r q_j$ is useful for assessing Gaussianity -- that is whether the copula  is Gaussian.   If $a_j+bq_j\approx rq_j$ then the quantiles predicted under the G and PG models are approximately the same and  a 45 degree line plot suggests a Gaussian copula is adequate.     This analog of the qq--plot is converted to the pp--plot analog  by transforming each coordinate using $\Phi$.   Using the inverse marginal distribution of each of the variables displays the copula fit information in the original scale. 

For the Gaussian copula the single parameter is $0\le\rho\le 1$, estimated by  $r$.   The analogous estimate for the PG copula is $R=\pm\sqrt{1-(s\hat\sigma)^2}$ signed according to $b$.   A measure of the  contribution of the nonparametric component $\alpha_j$  to the PG fit is  
$
R^2-r^2 = 1-r^2 - (s\hat\sigma)^2
$.
A more formal analysis is based on the log--likelihood where in $\eref{cusp}$ the disturbance terms $\eps_j$ and $\eta_j$ are assumed normally distributed.    Then  minus twice the log--likelihood\footnote{This is the log--likelihood $z_j\sim N(\rho q_j,1-\rho^2)$ and differs from the bivariate Gaussian log--likelihood.  It appears  as  $s\rightarrow\infty$, $s\hat\sigma\rightarrow\sqrt{ 1-b^2}$ and so the G copula appears enforced.}  of the Gaussian and  PG model  are computed and their difference compared to a chi--squared distribution with 2 degrees of freedom for significance.

Local  measures of departure from Gaussianity are 
\be{lr}
\frac{z_j-r q_j}{h_j\sqrt{1-r^2}}\cq \frac{z_j-a_j-bq_j}{ s\hat\sigma h_j}\ .
\ee
The  standardised residuals
provide insight into areas of lack of fit, with mean and standard deviation, across $t$,  of 0 and 1, respectively.

\section{Tuning PG to particular parts of the joint distribution}
The function $h_j$ is used to assign  importance to parts of the joint of distribution.  If all parts are equally important then $h_j=1$ for all $t$.   If $h_j\approx 0$ when $t\approx n$ then fidelity  is enforced in the upper tail.  Alternatively if $h_j\approx 0$ for $t\approx n/2$ then there is an attempt to accurately reproduce $z_j$ in the middle of the distribution with smoothness  in both tails.
\section{Overall and local correlation}

\cite{bjerve1993correlation} state ``The local regression slope, which focuses on change in expected $y$ as $x$ changes, may be of greater interest than expected  $y$ for a given $x$." They go on to argue for  a  ``scale--free standardised version of the local regression slope."\footnote{The scale free feature conflicts with the calculations below where  slope differs according to whether it is with respect to $t$ compared to $q_j$.  Hence the noise terms must also be affected by the transformation in a way not apparent in the expressions below.} In terms of the current notation, their suggested measure is 
\be{rhot}
 \rho_t\equiv\pm\left\{1+\left(\frac{\psi_t}{s\sigma h_t}\right)^{-2}\right\}^{-\frac{1}{2}}\ ,
\ee
where $s$, $\sigma$ and $h_t$ are as in \eref{cusp} or \eref{spline} and $\psi_t$ is the regression slope at $t$ as set out below.  The ratio $\psi_t/(s\sigma h_t)$ is a signal to noise ratio and a high value implies $\rho_t\approx \pm 1$.    In general the signal  $\psi_t$ is the local slope of the relationship while $s\sigma h_t$ is a measure of local noise.   The sign of $\rho_t$ is discussed in the context of particular choices of signal $\psi_t$.  

With the Gaussian copula 
$$
z_t=\beta q_t + (\sqrt{1-\rho^2})\varepsilon_t\cq \eps_t\sim N(0,1)\ .
$$
In this  case the nonparametric component $\alpha_t$ is absent.  The slope with respect to $q_t$ is constant $\psi_t=\beta=\rho$ as is the noise variance: $s\sigma h_t=\sqrt{1-\rho^2}$.  Further  \eref{rhot} reduces to $\rho_t=\rho$ for all $t$.  The Gaussian copula case is displayed in the first row of the of \tref{corr}.

\begin{table}[htdp]
\caption{Overall and local correlations.  In all cases $R=\sqrt{1-\Ex\{(z_t-\hat z_t)^2\}}$}\label{corr}
\begin{center}
\begin{tabular}{l|ccc|l}
\hline
case & $\psi_t$ & $q_t$ & $\alpha_t$ & estimates\\
\hline\hline
Gaussian & & & \\
-- overall,  local &$\rho=\beta$ &$\checkmark$& $\times$ & $r=\Ex(z_tq_t)$ \\
\hline
nonparametric & & & \\
-- overall &$\rho$ &$\times$ & $\checkmark$& $r$ from $\widehat{\mathrm{PG}}(\beta=0)$\\
-- local  &$n\phi(q_t)\delta_t$ &$\times$ & $\checkmark$& $\hat\delta_t$ from $\widehat{\mathrm{PG}}(\beta=0)$\\
\hline
PG & & & \\
-- overall & $\rho$ & $\checkmark$& $\checkmark$& $r$ from $\widehat{\mathrm{PG}}$ \\
-- local & $n\phi(q_t)\delta_t+\beta$&$\checkmark$& $\checkmark$&$\hat\beta$, $\hat\delta_t$ from $\widehat{\mathrm{PG}}$  \\
\hline\hline
\end{tabular}
\end{center}
\end{table}%

The second set of row in \tref{corr} is the nonparametric Gaussian copula where, in \eref{cusp},  $\beta=0$.  Hence $q_t$ is removed and  the relationship between $z_t$ and $t$ or $u_t$ is  modelled in terms of the  nonparametric component $\alpha_t$.  The   slope $\alpha_{t+1}-\alpha_t$  at  $t$ is, apart from error,
$\delta_t$.   However $\delta_t$ is the slope with respect to $t$ and required is the slope with respect to $q_t$:
\be{normslope}
\psi_t=\frac{\de \alpha_t}{\de q_t} = \left( \frac{\de u}{\de t}\frac{\de q_t}{\de u}\right)^{-1}\frac{\de \alpha_t}{\de t}  \approx
\left\{\frac{1}{n\phi(q_t)}\right\}^{-1}\delta_t  = n\delta_t\phi(q_t)\ ,
\ee
where $\phi$ is the standard normal density.    

The final set of rows in \tref{corr} deals with the general PG case.   As in the other situations, the overall correlation $\rho$ is derived from the overall fit.   The overall correlation exceeds that derived from both the specialised Gaussian and nonparametric cases.   The higher overall correlation is partialed out locally according to both nonparametric and parametric components.   In any PG fit $\hat\beta$ will differ from the Gaussian copula estimate $\Ex(z_tq_t)$.  

If $s=0$ then $s\eps_t=0$  and  the signal to noise ratio is infinite implying $\rho_t=1$ for all $t$.  Thus the empirical and comonotonic cases imply, as expected,  $\rho_t=\pm 1$.   With the linear probit model $\sigma=0$ and $\alpha_t=\alpha_0(1-2u_t)$ with slope with respect to $u_t$ of $-2\alpha_0$ and
\be{normslope2}
\psi_t =  \left( \frac{\de q_t}{\de u_t}\right)^{-1}\frac{\de \alpha_t}{\de u_t}  =
 -2\alpha_0\phi(q_t)\cq  \rho_t\equiv\pm\left[1+\left\{\frac{2\alpha_0\phi(q_t)}{s\sigma h_t}\right\}^{-2}\right]^{-\frac{1}{2}}\ ,
\ee
If $h_t$ is constant then  local correlation is  highest if  $q_t=0$ and monotonically declines towards zero as $q_t$ moves away from zero. 

The above $\psi_t$ and implied correlations are for the $(q,z)$ scale.  However the local correlations also apply on the $(u,v)$ scale since the transformation from $u$ to $q$ and $v$ to $z$ are the same inverse normal probability transform and slopes are identical\footnote{This also follows since local correlation is independent of scale.}
$$
\frac{\de v}{\de u} =\left(\frac{\de z}{\de v}\right)^{-1} \frac{\de z}{\de q} \frac{\de q}{\de u} = \frac{\de z}{\de q}\ .
$$ 
\section{Simulating from the PG copula} 

To explain PG simulation, consider initially  the Gaussian model.   Given data $(q_t,z_t)$, $t=1,\ldots,n$ where
$q_t=\Phi(u_t)$ and $u_t=t/(n+1)$ suppose 
\be{rhohat}
\tilde z_i\sim N(r \tilde q_i,1-r^2)\cq r=\Ex(z_tq_t) = \E(\rho|  z)\cq \tilde q_i\sim N(0,1)\ ,
\ee
where $z\equiv(z_1,\ldots,z_n)$ contains the observed values of $z_t$.
Simulating $\tilde q_i$ multiple times produces multiple $\tilde z_i$ while $r$  remains fixed.   To ensure  coverage, $\tilde q_i=q_i$,  $i=1,\ldots, n$.   The  $n$ pairs $(u_i, \tilde v_i)$ where $\tilde v_i=\Phi(\tilde z_i)$, $i=1,\ldots,n$  provide $n$ independent draws with   uniform  empirical distribution for the $u_i$ and approximately uniform empirical distribution for the $\tilde v_i$.   Repeatedly cycling over $i=1,\ldots,n$  produces an unlimited supply of  draws.

The PG generalisation of \eref{rhohat} is to repeatedly cycle over  $i=1,\ldots,n$ with 
\be{rhohat2}
\tilde z_i\sim N(a_i+bq_i, s^2\hat\sigma^2)\ , \ \ \ \  a_i=\E(\alpha_i|  z)\ , \ \ \ \ b=\E(\beta|  z)\ .
\ee
The $a_i\equiv\E(\alpha_i|z)$ and $b\equiv\E(\beta|z)$ and estimate of $\sigma$ are calculated with the Kalman Filter Smoother (KFS)  \citep{DeJong:91a} while $s$ is set to achieve desired smoothness.  Alternatively $s$ is estimated as discussed in \sref{xx}.  In  \eref{rho2} the $a_i$, $q_i$  and $b$ remain fixed across cycles, analogous to fixing $r$  in the  Gaussian copula simulation.  If $a_i=0$ for all $i$ then \eref{rhohat2} reduces to \eref{rhohat} since in this case $b=r$ and $s^2\hat\sigma^2=1-r^2$. 

Iterations \eref{rhohat} and \eref{rhohat2}  ignore  estimation variability in $r$ or $a_i$ and $b$.  This shortcoming is addressed by varying  $a_i$ and $b$  across  each cycle 
$$
\tilde a_i \sim N\{a_i,\cov(\alpha_i|  z)\}\cq i=1,\ldots,n\cq  \tilde b\sim N\{b,\cov(\beta|  z)\}\ ,
$$
and using these  in the first relation of \eref{rhohat2}  instead of $a_i$ and $b$.
The conditional covariance matrices are recursively calculated  with the KFS alongside $a$ and $b$, while consistent  draws  across $i$  are made with the 
simulation smoother extension  \citep{DeJong&Shephard:95}. 

\section{Ordering}

Suppose $z_{xt}$ is the percentile based z--score of series $x$ at index $t$.   In this section two common indexes are discussed:   time $t$ and ordering with respect to some other variable.    A first order approximation is $z_{xt} = \beta_xq_t + \eps_{xt}$ or in vector form $z_t=\beta q_t + \eps_t$ where $z_t$, $b$ and $\eps_t$ are vectors and $q_t$ scalar.   For the Gaussian copula $\beta=\rho$.

\section{Factor PG copula model}

The PG  model is stated in terms of  $q_t=\Phi^-(u_t)$ where $u_t=t/(n+1)$, $t=1,\ldots,n$.   The  role of $x$  is to order $z$:  the variable $x$ is permuted  and $z$ is permuted using the same permutation.   This suggests using a composite variable for ordering.

For example suppose $x_t$ is a vector of variables initially ordered in terms of time $t$.   The associated percentile based z--scores  are   $z_t= (\Phi^{-1}\circ \P) x_t$ where $\P$ is the generic percentile operator.\footnote{A final adjustment is made to ensure the z--scores have mean zero and variance 1.}   Write $z_t=Uc_t$ where $(c_1,\ldots,c_n)=DV'$ derived from the singular decomposition $UDV'=(z_1,\ldots,z_n)$ arranged so that the singular values in the diagonal matrix $D$ are in decreasing order.   Use the first component in $c_t$, corresponding to the largest singular value,  to permute $x_1,\ldots,x_n$ and fit PG copulas
$$
x_t = \alpha_t + \beta q_t + sh_t\eps_t\ ,
$$
where $\alpha_t$ and $\beta$ are vectors and $h_t$ is a diagonal matrix.  The components of $\alpha_t$ are individual departures from the Gaussian component and $\beta\beta'$ models the joint Gaussian structure.






Suppose $y_t$ is a $p$--vector with components $y_{it}$, $i=1,\ldots,p$ with $y_t$ is observed for $t=1,\ldots n$.   Suppose each component series in $y_t$ is ordered according to $x_t$, a scalar series observed or inferred along with the $y_{it}$.   Without loss of generality assume $y_t$, $t=1,\ldots,n$ denotes the component series after ordering with respect to $x_t$ and define $v_t$ and $z_t$ from the componentwise ordered $y_t$ as, for $t=1,\ldots,n$, 
$$
z_t = \Phi^-_*(v_t)\cq v_t = F_*(y_t)\cq q_t=\Phi^-(u_t)\cq u_t=\frac{t}{n+1}\ .
$$
Using this notation the factor PG model is as in \eref{cusp} except that $z_t$, $\alpha_t$, $\beta$ and $\eps_t$ are vector and $h_t$ is a diagonal matrix. The diagonal matrix $h_t$ distributes the quantum of error $\sigma$  across component series $i$ and percentiles $t$.   The matrix $h_t$ is standardised such that $1'h_t1=p$.    Further $\alpha_t$ is formed as in the second and third equations in \eref{cusp} expect that both $\alpha_t$ and $\delta_t$ are vector.    More specific structure is: 
\begin{itemize}
\item   If all components of $\alpha_t$ are identical then  $\alpha_t$ is written as $1\alpha_t$ where in the last expression $\alpha_t$ is scalar.  In this formulation deviations from Gaussianity are the same for each series.  This is testable in the current framework.
\item If all components of $\beta$ are the same then  $\beta$ is written as $1\beta$ where $\beta$ is scalar.   In this case all components in $z_t$ respond in the same way to the Gaussian component $q_t$ but deviations may occur.  
\item A generalisation is where $z_t=B_t\alpha_t + Q_t\beta + sh_t\eps_t$.

\end{itemize}


Replacing $1\alpha_t$ in \eref{cusp} by a vector  $\alpha_t$ and making $s\sigma$ component specific  yields the PG model.  Variants include forcing  a common response to $q_t$ by requiring  $\beta$ to be scalar. 

The one--factor PG model is estimated in the same way as the PG model, using vector $z_t$.   In practice $x_t$, the scalar variable serving to order the components of $y_t$, is either an external driver or a composition from $y_t$.   At present $q_t$ is structured in terms of the normal score derived from the percentile of $x_t$.   More extreme tail behavior is modelled by  for example replacing the normal score $q_t$ by say $\chi_t$ the chi--squared score derived from the percentile.   This is explained in detail in the next section.

\section{Extreme tail copulas} 
 

\section{Below here is review/junk}

\subsection{A  model for monitoring slope}
Suppose
\be{slope}
y_t = \beta m_t + \mu_t + s\eps_t\cq \mu_{t+1} = \mu_{t-h+1} + hr_t + c\eta_t \cq r_{t+1}=r_t + \eta_t\ ,
\ee
where $y_t$ is a stock price,  $m_t$ is a market index, $\mu_t$ is the price level and $r_t$ is the return per period and $h$ is a positive  integer.   Note $r_t$ measures  the expected  per period change in the level $\mu_t$.   Thus

\subsection{Kelly criterion}

Suppose $r$ is a vector of stock returns with mean $\mu$ and covariance matrix $C$.    Suppose proportion $\pi_i$ of total wealth $w$  is invested in stock $i$.  Then the rate of return on the portfolio is 
$ \pi'r$.   The expected  log wealth after one period   is 
$$
\E[\ln \{w(1+\pi' r)\}] \approx \ln(w) + \pi'\E(r) -\frac{\pi'\E(rr')\pi}{2} =  \ln(w) + \pi'\mu -\frac{\pi'(C+\mu\mu')\pi}{2}\ .
$$ 
To maximise log wealth (and hence wealth), maximise this expression with respect to $\pi$.   Setting up the Lagrangian, differentiating and equating to zero yields 
$$
\pi = (C+\mu\mu' - \lambda I)^{-1}\mu\cq 1'\pi=1\ .
$$
Given the optimal $\pi$ the growth rate is 
$$
\E\ln(1+\pi'r) \approx \mu'(C+\mu\mu' - \lambda I)^{-1}\mu - \frac{\mu'(C+\mu\mu' - \lambda I)^{-1}}{2}
$$

The factor $1+\pi'r$ is the growth factor.   Over $n$ periods the growth factor is
$$
\e^{\ln(1+\pi'r_1)+\ln(1+\pi'r_2)+\cdots \ln(1+\pi'r_n)} \rightarrow \e^{n\E\ln(1+ \pi'r_t)} = \e^{nm}
$$
where $r_t\sim(\mu,C)$ are the vectors of growth rates for each period and $\pi$ is the fixed allocation.   Hence the rate of growth is
$$
m=\E\ln(1+\pi'r_t) 
$$

\subsection{Continuous formulation}
For a single stock we have $\de s/s = \mu\de t + \sigma\de b$ where $b$ is brownian motion.  Alternatively 
$\de s=\mu s\de t + \sigma s\de b$.    For a portfolio $\pi$ of stocks this becomes
$$
\frac{\pi'\de s}{\pi s} =\pi'\mu\de t + \pi'\de b
$$
$$
\pi=-C^{-1}(1r_f-\mu)
$$
Note $C^{-1}(r-\mu)$ is the standardised prediction error when predicting each component of $r$ using all the other components and supposing $r$ is realised.   Since $\mu>1r_f$,  outcomes $r=1r_f$ are downside surprises.
Thus  $\pi$ is the negative of the standardised prediction error when predicting each component of $r$ using all other components and assuming the downside surprise $r=1r_f$.   A high value $\pi_i$ occurs if  there is huge downside surprise if $r_i=r_f$   when all other components  downward surprise at  $r_f$.    Hence the Kelly criterion loads the portfolio with stocks $i$ which have  large downward surprise at $r_i=r_f$ if all other stocks downward surprise with outcomes $r_f$.      



\subsection{Markowitz}

Suppose we wish to maximise the expected return $\pi'\mu$ subject to a constraint on the volatility, $\pi'C\pi\le c$.  Then the Lagrangian is 
$\pi'\mu +\lambda (c-\pi'C\pi)$.   Differentiating and equating to zero yields the first order conditions
$$
\mu = 2\lambda C\pi\qquad \Rightarrow\qquad \pi = \frac{1}{2\lambda} C^{-1}\mu 
$$
where $\lambda$ is chosen so that $\pi'C\pi=c$. 
 

 If $C=c11'$ then

To maximise the expected growth rate requires $\pi$ to be chosen as above.  Can we write a SSM for this?
$$
1=1'\pi\cq s_t=1\mu + r_t + \eps_t 
$$


Multiplying the first equation by $1'$ yields
$$
\pi=n\le \cq\lambda=n1'\mu - n1'(C+\mu\mu')\pi\cq  
$$
$$
\mu = (C+\mu\mu')\pi\qquad\Rightarrow\qquad \pi=(C+\mu\mu')^{-1}\mu
$$
Maximising the expected log--accumulation yields the first order conditions
$
\E(r_t) = \E(r_tr_t')\pi
$ or $\pi=\{\E(r_tr_t')\}^{-1}\E(r_t)$.  If one the components of $r_t$ is constant (the risk free rate of return $\delta$ say), then (using partitioned inverses)
$$
\pi = \{\cov(r_t)\}^{-1}\{\E(r_t)-\delta 1)\ .
$$





\subsection{\cite{bjerve1993correlation} review}
\begin{enumerate}
\item  Points out $\cov(z|t=t^*)\ne 0$  while $\cov\{\E(z)|t=t^*\}=0$.  So cannot form a correlation from the variation at $t$ compared to the variation in the mean -- there is no variation in the latter.
\item  Conditioning $t$ on a neighbourhood of $t^*$ and calculating the correlation in neighbourhood will lead to low correlation even if relationship is strong.  Not sure I understand this.
\item  Distinguishes between $\cov\{\E(z|t)\}/\cov(z)$ as a measure of correlation and ordinary correlation.  The former he calls Pearson correlation while Gallton--Pearson is the usual correlation.
\item  States that to the first order  both correlations are such that
$$
\cor\{z,t|t\in N_h(t^*)\}=\frac{h\beta_{t^*}}{\sqrt 3\sigma_{t^*}}
$$
where $\beta_t$ is the local regression and $\sigma_t$ the local residual? variation.
\item Then considered is
$$
\xi_{t^*} = \lim_{h \rightarrow 0}\frac{\cor\{z,t|t\in N(t^*)\}}{h/\sqrt 3} = \frac{\beta_{t^*}}{\sigma_{t^*}}
$$
that is the local slope devided by the local standard deviation.  Note I  assume the overall correlation is 1 and so $\sigma_1$ does not appear in the numerator.
\item  Maps $\xi_{t^*}$ into the unit interval
$$
\rho_{t^*}= \pm\left(1+\xi^{-2}_{t^*}\right)^{-\frac{1}{2}}= \pm\left(1+\frac{\sigma^2_{t^*}}{\beta^2_{t^*}}\right)^{-\frac{1}{2}}\ ,
$$

\end{enumerate}


 

  


\section{Fitting PG copulas}
 The PG is readily  fit to percentile data, and provides a practical platform for copula simulation.  This is discussed, implemented, and used in subsequent sections.  The  parameters defining a PG copula are $s$, $\beta$,  $\sigma$ and starting conditions $\alpha_0$ and $\delta_0$.  Given $s$, the other parameters have  closed form expressions calculated with the Kalman filter.  A best estimate of $s$ is derived by iterating over $s$ so as to minimise  an appropriate objective function such as least squares. 



To compute local correlation, write $\Delta z_t\equiv z_{t+1}-z_t$ and similarly for other quantities.    Assume $t$ is given and note that 
$$
\Delta z_t =\delta_t +c\eta_t + s(\eps_{t+1}-\eps_t)+\beta\Delta q_t  \cq \Delta q_t\approx \frac{q'_t}{n+1} = \frac{1}{(n+1)\phi(q_t)}
$$
Hence
$$
\E(\Delta z_t) \approx \E(\delta_t) + \frac{\E(u_t)}{\phi(q_t)} 
$$
  Then $q_t$ is a random variable and 
$$
\cov(\Delta z_t,\Delta q_t|t) = \beta^2\cov(\Delta q_t|t)\approx (\beta q'_t)^2\cov(u_t)= \frac{(\beta\sigma)^2\pi\e^{q_t^2}}{6} 
$$$$
\cov(\Delta z_t|t) = (1+c^2 + 2s^2)\sigma^2 + \beta^2\cov(\Delta q_t|t) 
$$
This final expression assumes $\cov(\delta_t)=(n+1)\sigma^2/2$:  a random $t$ is chosen to observe the random walk. 
Hence the local correlation is 
$$
\frac{\cov(\Delta z_t,\Delta q_t)}{\sqrt{\cov(\Delta z_t)\cov(\Delta q_t)}} = \left\{1+\frac{A}{\beta^2\cov(\Delta q_t)}\right\}^{-1/2}
$$ 

The local correlation between $z_t$ and $q_t$ is 
$$
\rho_t\equiv\cor(\Delta z_t,\Delta q_t|t) =  \frac{\beta\cov(\Delta q_t)}{\sqrt{\cov(\Delta z_t|t)\cov(\Delta q_t)}}=\pm\left\{\frac{\cov(\Delta z_t|t)}{\beta^2\cov(\Delta q_t)}\right\}^{-1/2} 
$$
where ``$|t$" indicates conditioning on information up to and including time $t$.   A calculation shows writing $c=4-\sqrt 3$,
$$
\cov(\Delta z_t|t)  = (2s^2+c)\sigma^2+\beta^2\cov(\Delta q_t)\cq \cov(\Delta q_t) \approx (q'_t)^2\cov(u_t)= \frac{\sigma^2\pi\e^{q_t^2}}{6}
\ .
$$
Hence
$$
\O(\rho_t) = \frac{\beta}{s_t\sigma }
\cq 
s_t = \sqrt{\frac{6(2s^2+c)}{\pi}}\e^{-q_t^2/2}\ .
$$
Divisor $s_t$ is small if $|q_t|$ is large indicating the model posits higher tail correlation than normal if $\beta\ne 0$.  The ratio
$$
\frac{\O(\rho_t)}{\O(\rho)} =\frac{s}{s_t}= \sqrt{\frac{\pi}{6(2+c/s^2)}}\ \e^{q_t^2/2}\rightarrow \frac{\sqrt{2\pi}\ \e^{q_t^2/2}}{\sqrt{24}}\ ,
$$
compares local  to overall correlation.  If $s\rightarrow\infty$ local correlation in the tails  will far exceed overall  correlation.  This is obvious given $s\rightarrow\infty$ implies the model will exactly reproduce the data.  If $s\rightarrow0$

\subsection{Alternative view of local correlation}
In \eref{cusp},  a large $|\beta|$ suggests more extreme tail behaviour than in the Gaussian copula.  To see this note  
 \be{change}
\Delta \E( \alpha_t + \beta q_t)  \approx \frac{-2\delta_0}{n+1} + \frac{\beta}{(n+1)\phi(q_t)}= \frac{\beta/\phi(q_t)-2\delta_0}{n+1}\ ,
 \ee
 where $\Delta$ denotes change with respect to $t$.
Change \eref{change} is relatively large if $q_t^2$ is large, that is $t$ is near 1 or $n$. Higher than normal tail dependence implies $|\beta|$ large.  For a Gaussian copula, $\beta=\pm\sqrt{1-(s\sigma)^2}$ and comparing the ratio of estimates of the two sides to 1 is a test for Gaussianity.

\subsection{Correlation between $z_t$ and $\alpha_t$}


 
 
 
 
 The term $\alpha_t+\beta q_t$ is the signal and $\eps_t$, noise.    The signal is relatively smooth and contains both a nonparametric and Gaussian component $q_t$.   Departures of $z_t$ from the Gaussian component  are the sum of a persistent component $\alpha_t$ and  transient component, $s\eps_t$.  Since  $\eps_t$ and  the driver of persistence $\eta_t$  share $\sigma$, the smoothing parameter $s$ measures relative importance of transience as opposed to persistence.   The scale parameter $\sigma$ relates to the degree of roughness since it is the variance of the changes in the  slope of $\alpha_t$.
 

 
  Since $\Ex(q_t)=\Ex(z_t)=\Ex(\eps_t)=\Ex(\eta_t)=0$ it follows  
 $$
0= \Ex(\alpha_t)=\alpha_0+\delta_0 \Ex(t) = \alpha_0+\delta_0 \Ex(t) = \alpha_0+\delta_0 \frac{n+1}{2}\qquad\Rightarrow\qquad \delta_0 = \frac{-2\alpha_0}{n+1}\ .
 $$ 
Further $\Vx(q_t)=\Vx(z_t)=1$ implies an analysis of variance type decomposition
\be{anova}
 1= \Vx(\alpha_t) + \beta^2 + (s\sigma)^2\ .
\ee
Decomposition \eref{anova} splits the total variance  of 1 into that due to the nonparametric component $\alpha_t$,  Gaussian component $\beta^2$ and residual noise, $(s\sigma)^2$.  Alternatively \eref{anova} states the variance of the nonparametric component is $1-\beta^2-(s\sigma)^2$.

An example PG copula with $s\sigma=xx$, $\sigma=0$, $\beta=xx$ and $\alpha_0=xx$  is displayed in \fref{xx}.  
 \begin{comment}
 relation is such that
$$
\delta_0=\frac{-2\alpha_0}{n+1}\cq z_t=  \alpha_0(1-2u_t)+\beta q_t + s\eps_t \ .
$$
Additionally $\Vx(q_t)=\Vx(z_t)=1$ implying
\be{constraint}
 1= \frac{\alpha_0^2}{3}-2\alpha_0\beta c+\beta^2+(s\sigma)^2\cq c\equiv\cov(u_t,q_t)\approx 2.82\ ,
\ee
and hence at most two of $\alpha_0$, $\beta$ and $s\sigma$ can be chosen freely. 
\end{comment}
 
 



\section{Correlations with the PG copula}
With \eref{cusp}  and $t$ uniform on $1,\ldots,n$ 
$$
\Vx\left(\begin{array}{c} z_t\\ \alpha_t\\ q_t\\ \end{array}\right)
=\left(\begin{array}{ccc} 
1 & 1-(s\sigma)^2 & \beta\\
1-(s\sigma)^2 & 1-\beta^2-(s\sigma)^2 & 0\\
\beta & 0 & 1\\
\end{array}\right)\ .
$$
This covariance matrix serves to define a variety of ordinary and partial correlations.

The correlation between $z_t$ and $q_t$ is  $\beta$.  The partial correlation between $z_t$ and  the Gaussian component $q_t$, after removing the effect of the nonparametric component $\alpha_t$,  is called the Gaussian correlation:
\be{rho}
\rho_q\equiv\frac{\cov(q_t,z_t-\alpha_t)}{\sqrt{\cov(q_t)\cov(z_t-\alpha_t)}} = \frac{\beta}{\sqrt{\beta^2+(s\sigma)^2}}=\pm\left\{1+\left(\frac{s\sigma}{\beta}\right)^2\right\}^{-1/2}\ ,
\ee
where the sign is that of $\beta$.  Hence $\rho_q=1$ or 0 if  $s\sigma=0$ or $\beta=0$, respectively.
Using \eref{anova},  $\rho_q^2$ equals the proportion of  residual variance $\Vx(z_t)-\Vx(\alpha_t)=\beta^2+(s\sigma)^2$ explained by  the Gaussian component $q_t$.   
 

\begin{comment}
$$
= \cor(\alpha_t+s\eps_t,\alpha_t) =\frac{1-\beta^2-(s\sigma)^2}{\sqrt{(1-\beta^2-(s\sigma)^2)(1-\beta^2)}}= \left\{\frac{1-\beta^2-(s\sigma)^2}{1-\beta^2}\right\}^{1/2} 
$$
\end{comment}
and  $\rho_a$ is 1 or 0 according to whether  $s\sigma$ equals 0 or  $\sqrt{1-\beta^2}$, respectively. 
\begin{comment}
$$
\cov(z_t-\alpha_t) = \cov(\beta q_t+s\eps_t) = \beta^2 +s^2\sigma^2\ .
$$
\end{comment}
Finally the overall correlation is the correlation between $z_t$ and the signal, consisting of both nonparametric and Gaussian component:
$$
\rho\equiv\cor (z_t,\alpha_t+\beta q_t)=\cor(z_t,z_t-s\eps_t)=\frac{1-(s\sigma)^2}{\sqrt{\cov(z_t-s\eps_t)}}=\sqrt{1-(s\sigma)^2}\ .
$$
with $\rho=\rho_a$ if $\beta=0$.   If $s=0$, there is no measurement error in \eref{cusp} or, equivalently, there is no roughness penalty in \eref{spline} and $\rho_q=\rho_a=\rho=1$.

\begin{comment}
reducing to \eref{rho} except that $s$ is replaced by
$$
v\equiv \Ex\left(\sum_{j=1}^{t-1}w_j^2\right) = xxx \cq \alpha_t=\sum_{j=1}^{t-1} w_j\eta_{t-j}
$$
The derivation of $v=xxx$ is shown  appendix.  Equivalently $\rho_v^{2}$ is the proportion of $\Vx(z_t)$ explained by the nonparametric and Gaussian component.


From \eref{rho} 
\be{OR}
\left(\frac{v}{s}\right)^2=\frac{\rho_s^2/(1-\rho^2_s)}{\rho_v^2/(1-\rho_v^2)} \equiv \mathrm{OR} \ ,
\ee
the odds ratio of local to global correlation.  In general $v>s$ implying $\mathrm{OR}>xx$.   

\end{comment} 

\begin{comment} 
$$
\rho = \cor(z_t,q_t) =\frac{\beta}{\sqrt{\beta^2+\cov(\alpha_t)}}=\pm\left\{1+\left(\frac{v\sigma}{\beta}\right)^2\right\}^{-1/2}\ .
$$
The expressions follow from a detailed calculation
$$
\cov(z_t,q_t) = \E\{(\alpha_t+\beta q_t)q_t\} = \beta
\cq
\cov(z_t) =\cov(\alpha_t)+\beta^2\cq \cov(q_t)=1
$$
where .   A detailed calculation shows
\end{comment}




\section{Special cases of the PG copula}

\tref{pp} sets out possible choices for $s$ and $\beta$.   These combinations are discussed in the further subsections.  Note that $s=0$ does not imply $s\sigma=0$ as $s\rightarrow 0$ may imply $s\sigma$ converges to a positive number.



\subsection{Empirical copula}

The empirical copula results in \eref{cusp} when $s=0$.  In this case   $z_t=\alpha_t+\beta q_t$.    The deviation of $z_t-(\alpha_t+\beta q_t)$ can be made zero by selecting appropriate $\eta_t$.  Hence $s=0$ implies the fitted copula is the empirical copula.  Note $\Vx(\alpha_t)=1-\beta^2$ and since $\beta$ is arbitrary, the correlation between $z_t$ and the nonparametric component is 1.

\subsection{Large local variation}

The other  extreme  $s\rightarrow\infty$ is displayed  in the bottom rows of \tref{pp}.  Large $s$ forces negligible $\sigma$  and straight line behaviour for $\alpha_t$ yielding
$$
z_t= \alpha_0(1-2u_t) + \beta q_t + s\eps_t\ ,
$$
where $s\eps_t$ has finite variance $(s\sigma)^2$.     

\subsection{Gaussian copula}
If $s\rightarrow\infty$ and  $\alpha_0=0$ then $(q_t,z_t)$ is bivariate   Gaussian with correlation $\beta$ and \eref{cusp} reduces to a Gaussian copula with correlation $\beta$ and $\rho=\rho_q$.  In this case $s\sigma=\sqrt{1-\beta^2}$. 

\subsection{Probit copula}


 If $s\rightarrow\infty$ and  $\beta=0$ then \eref{cusp} defines a probit copula.   From \eref{constraint}, $\alpha_0=\pm\sqrt{ 3\{1-(s\sigma)^2\}}$. 
 
 \subsubsection{Independence}
 If $s\rightarrow\infty$ and  $\alpha_0=\beta=0$ then the variables are independent.   The situation is achieved either letting $\beta\rightarrow 0$ with a Gaussian copula or letting $\alpha_0\rightarrow 0$ with a Probit copula.  Under independence $s\sigma=1$.  
 
 \subsection{Conditionally Gaussian copula}  

This is the intermediate situation where  $0<s<\infty$.  Testing $\alpha_0=1/s=0$ is a test for a Gaussian copula.   Goodness of fit is measured with 
\be{R2}
\hat\rho^2= 1-(s\hat\sigma)^2=1-\Ex\{(z_t-\hat z_t)^2\}\ ,
\ee
where  $\hat z_t$ is the estimate of $z_t$ based on \eref{cusp} using estimates of $\alpha_0$, $\beta$ and $s\sigma$.  
Values of $\hat\rho^2$ lie between 0 and 1 and have the usual interpretation:    near 1 indicates the percentiles of $u$ well explain $z$ values of $v$.   
Note $R\rightarrow\beta$  if $\alpha_0=0$ and $s\rightarrow \infty$.

\section{Estimation of the PG model}

A convenient feature of the GC model is the straightforward estimation.   The unknown parameters are $\alpha_0$, $\beta$, $\sigma$, and $s$.  Given $s$, estimates of the other parameters are in closed form -- generalised least squares (GLS) estimators of $\alpha_0$ and $\beta$, while $\sigma$ is estimated from the fitted  residuals.   GLS estimates are derived using the Kalman filter (KF).   The KF  and the associated Smoothing filter (SF) also deliver diagnostics for assessing goodness of fit and estimates of  sampling error.  In these calculations $s$ is either given or alternatively is iterated over to produce a quasi maximum likelihood estimate.

From \eref{R2}
$$
\Ex\left\{ \left(\frac{z_{t}-\hat z_{t}}{s\hat\sigma}\right)^2\right\}=1
$$
suggesting  the comparison of standardised residuals  to $\pm 2$ and perhaps plotting the percentile rank of the standardised residuals against $u_t$ to identify  lack of fit.  

\begin{comment}
Note $R^2$ differs from the correlation coefficient $\beta$ since.
$$
\frac{R}{\beta}=\pm \sqrt{1-(s\hat\sigma)^2}\left\{1+\left(\frac{s\sigma}{\beta}\right)^2\right\}
$$
This emphasises the difference between $R^2$ and $\beta$.   The former measures the goodness of fit of a semiparametric function of $u$ to explain $z$ while $\beta$ measures the ability of a straight line in $q$ to explain $z$.  If $\beta\ne 0$ then $q$ forms part of the semiparametric specification and hence if $\beta\ne 0$,  $R^2>\beta^2$. 
\end{comment} 

   The implied correlation between $z_t$ and $q_t$ is  
$$
\beta=\frac{\beta}{\sqrt{\beta^2+(s\sigma)^2}} = \pm\left\{1+\left(\frac{s\sigma}{\beta}\right)^2\right\}^{-1/2}\ .
$$
The term $\beta/(s\sigma)$ is the signal to noise ratio.  If $z_t, q_t\sim (0,1)$ then $\beta^2+(s\sigma)^2=1$ and  
$$
\beta=\pm\sqrt{1-(s\sigma)^2}=R=\beta \cq \frac{\beta}{s\sigma} = \frac{\pm\sqrt{1-(s\sigma)^2}}{s\sigma} = \frac{\beta}{\sqrt{1-\beta^2}} 
$$.


 Thus the empirical correlation between $z_t$ and $q_t$ is
$$
\sum_{t=1}^n z_tq_t = \beta+\delta_0\sum_{t=1}^n tq_t 
$$




By construction both $z_t$ and $q_t$ have mean zero and variance 1 implying
the correlation between $z_t$ and $q_t$ is


If  $\beta=0$ then 
$$
z_t=\alpha_0 + n\delta_0 u_t+\beta \Phi^-(u_t) +s\eps_t\ ,
$$
    Testing $\delta_0=0$ amounts to testing for independence.  If $\beta=\pm 1$ then 
$z_t-(\pm q_t)=\alpha_0+t\delta_0$ implying $\alpha_0=n\delta_0=0$ and  $z_t=\pm q_t$, that is the variables are co or counter monotonic.   Finally if $\beta\ne 0,\pm 1$ then $z_t=\alpha_0+t\delta_0+\beta q_t+s\eps_t$  since  $\alpha_0=\delta_0=0$, implied by the fact that    Since both $z_t$ and $q_t$ are, by construction, standard normal, this  amounts to prescribing  a Gaussian copula with correlation indicated in \tref{pp}.
The ratio $\beta/(s\sigma)$ is the  signal to noise ratio.

The middle column of the body of \tref{pp} displays alternatives for finite $s\ne 0$.   If $\beta=0$  the implied copula is nonparametric with $\alpha_t$ evolving smoothly as a function of $t$ with the actual degree of smoothness increasing with $s$.  A constraint is $\alpha_0=\alpha_n=\sum_t\delta_t=0$ and hence $\alpha_t$ forms a bridge.  If $\beta=\pm 1$ then the signed difference $z_t-(\pm q_t)$ is modelled as a smooth. In a fit, the  bridge structure is expected to be enforced by the data.

The scale  $\sigma$ quantifies roughness in the signal $\alpha_t$ but its size is determined by $s$ with $s\sigma\le 1$ implied by the standard normality of $z_t$.   
In fitting, $s$ is either chosen a--priori, or estimated together with the other parameters. Given $s$, estimates of  $\beta$, $\alpha_0$, $\delta_0$ and $\sigma$ are closed form generalised least squares calculations.    Thus $s$ can be estimated  using a one dimensional search.   Given the values of the parameters,  standard diagnostics are used to assess the fit and reveal areas of inadequate fit.  These techniques are used and explained in the applications.



Use of $q_t$ permits  a partially Gaussian explanation of the copula with  Gaussianity increasing with $s$.   In the limit the enforced Gaussian copula has correlation $R$ signed according to $\hat\beta$.  Co or  countermonotoniticity implies $R=\pm 1$ with the sign indicating the direction of the relationship.  Setting $\beta=0$  excludes the Gaussian copula  component.



\begin{table}[htdp]
\caption{Predicted $z_t$ with conormal copula model}\label{ppp}
\begin{center}
\begin{tabular}{c||c|c|c}
\hline
&$s=0$& $0<s<\infty$ &$s\rightarrow\infty$\\
\hline\hline
$\beta=0$&& $\alpha_t$&$\alpha_0(1-2u_t)$ \\
& & non parametric& linear probit\\
& & probit & independence: $\alpha_0=0$ \\
\cline{1-1}\cline{3-4}
$\beta\ne 0,\pm 1$  & $z_t$ & $\alpha_t+\beta q_t$ &$\alpha_0(1-2u_t)+\beta q_t$\\
&empirical & conormal & Gaussian if $\alpha_0=0$\\
\cline{1-1}\cline{3-4}
$\beta=\pm 1$& & $\alpha_t\pm q_t$& $\alpha_0(1-2u_t)\pm q_t$ \\
& & cointegrated & co/counter monotonic \\
& & departure & if $\alpha_0=0$ \\
\hline
\end{tabular}
\end{center}
\end{table}%


The fitted values $\hat z_t$ stacked in  vector $\hat z$  provide a basis for simulating all percentiles including extreme percentiles.   In particular suppose $z\sim N(\hat z, \Sigma)$ where $\Sigma$ is the covariance matrix implied by the model.   Then $v\sim\Phi_*(z)$ is a simulation of the the $v_t$.   Bringing $v$ into the original order yields $(u_t,v_t)$, a simulation from the copula.  Simulated percentiles are  mapped back into the original scale as   $F_*^-(u_t,v_t)$ to provide simulated $(x_t,y_t)$ outcomes.  

\section{Copula forcing}

Even if data suggests a strict Gaussian copula it  may be desirable to force stronger dependence, especially in the tails. This forcing is achieved with 

\section{Application to copula fitting}

The following example illustrates the copula fitting technique.




\section{Financial sensitivity and contagion}
 

Suppose the singular value decomposition of the  matrix $S$ of sensitivities at a particular $q$ is 
$S= UDV'$.  Here  $D$ a diagonal matrix of singular values, arranged in decreasing order along the diagonal and $U$ and $V$ are orthonormal:  $U'U=V'V=I$.  If the series are independent then $U=D=V=I$.   If the series are comonotonic then $U=V=(1,0)$ while $D=\diag(1,0,\ldots,0)$.  Hence in the comonotonic case $S=11'$.

So the best thing is to compare 
$$
Q= s_{i1}c_{j1} + \cdots + s_{ip}c_{jp} 
$$

If $Q=I$ then $S=C=I$.   If $Q=11'$, a matrix of ones, then $S=\sqrt{p}(1,0)$ and $C=(c,0)$ both have rank 1:  here $s$ and $c$ are the leading columns of $S$ and $C$, respectively and 0 a matrix of zeros.

The approximation $Q\approx sc'$ is exact if $Q$ has rank 1 implying $Q=11'$.   The approximation $Q\approx sc'$ is inadequate if $Q=I$.  Inadequacy can be judged from the largest singular value in $D$, as a proportion of $p$, the number of variables or rows in $Q$.

$$
Q_* \equiv \frac{1}{p-1}(Q-I)=UDV'\cq s\equiv Q_*1\cq c\equiv Q_*'1\ .
$$$$
Q=I + s1'+ UDV'\cq s\equiv \frac{1}{p-1}(Q-I)1
$$
Then $s$ is the vector of average \v\ sensitivity of each variable to all others.  Further $c$ is the average \v\ impact of each variable on all others.   The matrices $U$, $D$ and $V$ define the singular value (svd) decomposition of $Q_*$, arranged so that diagonal matrix $D$ has the singular values on  the diagonal in descending order.   If $b=d_1u_1$ and $c=v_1$ where $u_1$, $d_1$ and $v_1$ are the first column, top diagonal entry, and first column of $U$, $D$ and $V$ respectively, then  for two variables $y\ne x$ in $Q$,
$$
\Delta_x q_y = s_y+ b_yc_x +\eps_{yx}
$$
This states that percentage sensitivities are, apart from the ``error" $\eps_{yx}$, an average sensitivity plus a scaled response to the contagious effect of the $x$ variable.   The contagious effects contained in $c$ are estimated by maximising the explanation of $Q$.


The vector $1'Q$ sums  the changes in \v\ when each of the column variables is stressed, and writes this as a proportion of the change in the variable being stressed.   These proportional sums, subtracting 1 and divided by  $p-1$ where $p$ is the number of variables, measures the average contagion of each variable on all others:  $c'=(p-1)^{-1}1'(Q-I)$ or $c=(p-1)^{-1}(Q-I)'1$

Alternatively the vector $Q1$ sums the changes in \v\ of each row variable when all the column variables are stressed.   Again it is appropriate to remove the effect of a variable on itself and consider the average over the remaining variables:  $s\equiv(p-1)^{-1}(Q-I)1$.  If $Q_*\equiv (p-1)^{-1}(Q-I)$ then $s=Q_*1$ and $c=\dot q_*1$ are the vectors of sensitivities and contagions, respectively.  If $Q=I$ then $s=c=0$.   If $Q=11'$ then $s=c=1$.



 
 \section{Systemic risk and causal chains}
A rank one approximation to the matrix  $Q$ is 
 $
 Q  \approx  sc' 
 $.
 Vector $c$ is an index of the contagious  impact of each of variables on the others while $s$ measures the sensitivity of each variable to each of the others.  The vectors $s$ and $c$ are derived from the singular value decomposition $Q=UDV'$ where $U'U=V'V=1$  and  where $s$ and $c$ are the first column of $UD$ and $V$  respectively, assuming the svd is organised so that the singular values in the diagonal matrix $D$ are organised from largest to smallest.   The appropriateness of the summarisation $sc'$ is measured with $\tr(Q-sc')$.
 
If the variables are independent then $Q=I$ and both $s$ and $c$ equal a column of the identity matrix with $sc'$ a matrix of 0's except in a single diagonal position where it is 1.   Then all but one variable has a contagion effect and only that variable is sensitive to the contagion provided by the  variable.

If the variables are comonotonic then $Q=11'$ and $s=1$ and $c=1$ where 1 denotes a vector of ones.   Thus the rank 1 approximation is exact and each variable is equally contagious and equally sensitive.
 
 Note that 
 $$
 Q^{n} \approx (s'c)^{n-1}sc'=\ ,
 $$
 
 
 If the random variables are independent the $Q-I=0$ and $a=b=k=0$ and there is no error in the first order svd approximation.    If the random variables are comonotonic then $Q=11'$ and $Q-I$ has ones everywhere except on the diagonal where it is zero.   The vector of row means is then $p^{-1}(p-1)1$
 
Systemic risk in the system is measured with $b'k=d_1(u_1'v_1)$.   In the case of comonotonic  random variables $Q=11'$, $a=1$ and $x$, where $p$ is the number of variables,  $d_1=1$. 
 
Furthermore we may define quantities such as
$
u^- \equiv \var_q(v|u\le q)
$
measuring the impact of a non distressed state in $v$.  For brevity we do not dwell on these constructs in this writeup although the ramifications and potential uses of these constructs will be  investigated in the research.

\section{Generalising Sklar's Theorem}

Skalr's Theorem states that if $F(x)$ is a a joint then there exists a $C$ such that
$
F=C\circ F_*
$ where $C_F=F$
\section{Literature}

We propose a measure for systemic risk: CoVaR, the value at risk (VaR) of financial institutions conditional on other institutions being in distress. We define an institution?s (marginal) contribution to systemic risk as the difference between CoVaR and the financial system?s VaR. From our estimates of CoVaR for characteristic-sorted portfolios of publicly traded financial institutions, we quantify the extent to which characteristics such as leverage, size, and maturity mismatch predict systemic risk contribution. We argue for macro-prudential regulation based on the degree to which such characteristics forecast systemic risk contribution.

\section{Econometric implementation}

The above development sets out our proposed  broad  framework for linking bivariate copulas and marginals to external variables and shocks study the impact of the same on stresses within the system and the contagious effects of crises.   Proposed econometric analysis will implement and extend  \cite{brownlees2010volatility}.

\section{Data}

We will employ publicly available data as published by APRA and other regulators.

\section*{References}
\bibliography{/users/pietdejong/documents/research/cifr/piet2}


\end{document}
